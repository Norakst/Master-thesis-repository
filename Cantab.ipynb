{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import heapq\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "sns.set_style(\"darkgrid\")\n",
    "from networkx.algorithms.community.quality import modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_IDs = [] # participant ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cantab_file(file, enc): # reads the file and returns a df. choose encoding.\n",
    "    df = pd.read_csv(file, encoding=enc)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vars(all_var, rem_var):\n",
    "    '''\n",
    "    Removes the variables that are a priori decided not to be included in the \n",
    "    framework from the CANTAB data. \n",
    "    task MOT, the standard deviation variables and other variables.\n",
    "\n",
    "    returns the reduced list of variables\n",
    "    '''\n",
    "\n",
    "    rest = [var for var in all_var if var not in rem_var]\n",
    "    rest = [var for var in rest if 'SD' not in var]\n",
    "    rest = [var for var in rest if 'MOT' not in var]\n",
    "\n",
    "    return rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = remove_vars(cols_var, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, visit, site):\n",
    "    \"\"\"\n",
    "    process the data, filter on visit session and site, remove unneeded columns, remove variables not to be included\n",
    "    \"\"\"\n",
    "    \n",
    "    sites = [1,2,3,4]\n",
    "    \n",
    "    df = df.loc[df['Visit Identifier'] == visit] # filtering out only sessions from visit identifier\n",
    "    \n",
    "    if site in sites:\n",
    "        df = df.loc[df['Site'] == site] # Filtering on site if relevant, else all sites are included\n",
    "        \n",
    "    all_col = df.columns.tolist()  \n",
    "    i = all_col.index('SWM Recommended Standard 2.0 Extended Observation')\n",
    "    cols_var = all_col[i+1:-1] # filtering out variable columns \n",
    "\n",
    "    # Removing variables not to be included: \n",
    "    variables = remove_vars(cols_var, var_to_remove+var_na)\n",
    "    print('variablene: '+ str(len(variables)))\n",
    "\n",
    "    var_with_ids = variables.copy() # making duplicate list of variable list for adding ID columns as well\n",
    "\n",
    "    var_with_ids.insert(0, 'Participant ID') # adding IDs first in the list \n",
    "    var_with_ids.insert(1, 'session_id')\n",
    "   \n",
    "    df = df[var_with_ids] # reducing the df with respect to variable list with IDs\n",
    "\n",
    "    df = df.reset_index(drop=True) #reseting the indexes\n",
    "    \n",
    "    return df, variables, var_with_ids # returning dataframe, list of all relevant variables and list of all variables + ids \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_both_dfs(df1, df2):\n",
    "    ids_1 = df1['Participant ID']\n",
    "    ids_2 = df2['Participant ID']\n",
    "    \n",
    "    print(len(ids_1))\n",
    "    print(len(ids_2))\n",
    "    \n",
    "    in_both = list(set(ids_1).intersection(ids_2))\n",
    "    \n",
    "    return in_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df(df_, cols, sc):\n",
    "    '''\n",
    "    Takes in a dataframe (can contain participant and session id), a list of variables for column names, name of scaler \n",
    "    scales df with standardscaling or minmax\n",
    "    assigns new df with given columns\n",
    "    returns scaled df with participant and session id\n",
    "\n",
    "    '''\n",
    "    df = df_.copy()\n",
    "    \n",
    "    if sc == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif sc == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    df_sc = scaler.fit_transform(df[cols])\n",
    "    df_sc = pd.DataFrame(df_sc, columns = cols)\n",
    "    df[cols] = df_sc \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_remove = [ # variables that are decided not to be included, bad measures, irrelevant measures or wrong representation/calculation\n",
    "    'DMSCC',\n",
    "    'DMSML',\n",
    "    'DMSML0',\n",
    "    'DMSML12',\n",
    "    'DMSML4',\n",
    "    'DMSMLAD',\n",
    "    'DMSMLS',\n",
    "    'DMSPEGC',\n",
    "    'DMSPEGE',\n",
    "    'DMSTC0',\n",
    "    'DMSTC12',\n",
    "    'DMSTC4',\n",
    "    'DMSTCAD',\n",
    "    'DMSTCS',\n",
    "    'DMSTEAD',\n",
    "    'DMSTEC',\n",
    "    'DMSTECAD',\n",
    "    'DMSTED',\n",
    "    'DMSTEDAD',\n",
    "    'DMSTEP',\n",
    "    'DMSTEPAD',\n",
    "\n",
    "    'PALTA12',\n",
    "    'PALTA2',\n",
    "    'PALTA28',\n",
    "    'PALTA4',\n",
    "    'PALTA6',\n",
    "    'PALTA8',\n",
    "\n",
    "    'PRMMCLD',\n",
    "    'PRMMCLI',\n",
    "\n",
    "    'RVPML',\n",
    "\n",
    "    'SWMDE4',\n",
    "    'SWMDE6',\n",
    "    'SWMDE8',\n",
    "    'SWMPR',\n",
    "    'SWMS',\n",
    "    'SWMS6',\n",
    "    'SWMSX',\n",
    "    'SWMTE4',\n",
    "    'SWMTE6',\n",
    "    'SWMTE8',\n",
    "    'SWMWE4',\n",
    "    'SWMWE6',\n",
    "    'SWMWE8'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_na = ['DMSMDL12', # removed when the number of nan-values were really high\n",
    " 'DMSML12',\n",
    " 'PALMETS28',\n",
    " 'SWMBE12',\n",
    " 'SWMDE12',\n",
    " 'SWMTE12',\n",
    " 'SWMWE12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeAdjMatrix(df, sim_metric, visit, save_adjM): #'Eucl', 'Cos', 'Pears'\n",
    "\n",
    "    n = len(df) # find number of rows (number of participant sessions)\n",
    "    adj_matrix = np.eye(n) # create adjacency matrix with 1s along diagonal\n",
    "\n",
    "    if sim_metric == 'Eucl':\n",
    "        distances = pdist(df.values, metric='euclidean') # Compute pairwise Euclidean distances between participants in a condensed array\n",
    "        sq_distances = squareform(distances) # Convert the condensed distance matrix into a square distance matrix\n",
    "        \n",
    "        # Fill the upper triangle of the similarity matrix with similarities:\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                adj_matrix[i, j] = 1 / (1 + sq_distances[i, j])\n",
    "\n",
    "    elif sim_metric == 'Cos':\n",
    "        similarities = cosine_similarity(df.values)\n",
    "\n",
    "        # Fill the upper triangle of the similarity matrix with similarities:\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                adj_matrix[i, j] = similarities[i, j]\n",
    "\n",
    "    elif sim_metric == 'Pears':\n",
    "        #df_trans = df.copy()\n",
    "        df_trans = df.copy().transpose() # Transpose the df so that participants are along the columns (necessary for the correlation function)\n",
    "        matrix_p = df_trans.corr() # Compute the similarity matrix with pearson correlation\n",
    "        sim_matrix_p = (matrix_p + 1)/2\n",
    "        adj_matrix = np.triu(sim_matrix_p.values) # Extract the upper triangular part of the array (including the diagonal)\n",
    "\n",
    "    if save_adjM:\n",
    "        np.savetxt(f'adjM_{visit}_{sim_metric}.csv', X=adj_matrix, delimiter=',')\n",
    "\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_PSN(adj, phi, s, title): \n",
    "    '''\n",
    "    Takes in adjacency matrix adj, cutoff phi and seed s\n",
    "    Creates and draws network graph with spring layout\n",
    "    \n",
    "    returns network and similarity list\n",
    "    '''\n",
    "    \n",
    "    G = nx.from_numpy_array(adj)\n",
    "    PSN = nx.Graph()\n",
    "    PSN.add_nodes_from(G.nodes)\n",
    "\n",
    "    similarities = []\n",
    "    n = len(G.nodes)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            sim = adj[i,j]\n",
    "            similarities.append(sim)\n",
    "            if sim > phi:\n",
    "                PSN.add_edge(i, j, weight = sim)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(title + ', cutoff= ' + str(phi))\n",
    "    pos = nx.spring_layout(PSN, seed = s, k=1/2)\n",
    "    nx.draw_networkx_nodes(PSN, pos=pos, node_size=80, alpha=0.7)\n",
    "    nx.draw_networkx_edges(PSN, pos=pos, width=2, alpha=1/10)\n",
    "    \n",
    "    plt.grid(False)\n",
    "    plt.gca().set_facecolor('white')\n",
    "    \n",
    "    return PSN, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarities(sim, title):\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Similarities')\n",
    "\n",
    "    sns.set(font_scale=2) \n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    sns.histplot(sim, bins=100)\n",
    "\n",
    "    #plt.axvline(np.mean(sim), color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    sorted_sim = sorted(sim, reverse=True)\n",
    "    top_10_index = int(0.1 *len(sorted_sim))\n",
    "    top_10_cutoff = sorted_sim[top_10_index]\n",
    "\n",
    "    plt.axvline(top_10_cutoff, color = 'k', linestyle='dashed', linewidth=2)\n",
    "    plt.text(top_10_cutoff+0.01, 500, str(round(top_10_cutoff, 3)),  va='bottom', ha='left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('mean: ' + str(np.nanmean(sim)))\n",
    "    print('Top 10 percent similarities: ' + str(top_10_cutoff))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apoe_df = pd.read_csv('Genotype_formatted_results.csv') #reads the APOE result file \n",
    "APOE_ids = list(apoe_df['Participant'])\n",
    "\n",
    "holdout_df = pd.read_excel('Hold_out.xlsx') # reads the hold out file with particiant IDs not to be included in analyses\n",
    "holdout = list(holdout_df['participant_id'])\n",
    "\n",
    "df = read_cantab_file(\"CANTAB-ALL.csv\", 'ISO-8859-1') # reads the cantab file\n",
    "\n",
    "# process the cantab df:\n",
    "df_1, cols_var, cols_var_ids = preprocess_df(df, 1, 'all') # df, visit, site\n",
    "\n",
    "\n",
    "# filter the ids common in all datasets:\n",
    "#cantab_ids = in_both_dfs(df_1, df_3) #finds the participant IDs in both visit dfs\n",
    "cantab_ids = list(df_1['Participant ID'])\n",
    "cantab_apoe_ids = list(set(cantab_ids).intersection(set(APOE_ids))) #finds common IDs with APOE file \n",
    "print(len(cantab_apoe_ids))\n",
    "\n",
    "relevant_ids = [id for id in matched_IDs if id not in holdout]\n",
    "#relevant_ids = [id for id in cantab_apoe_ids if id not in holdout]\n",
    "\n",
    "df_1 = df_1[df_1['Participant ID'].isin(relevant_ids)]\n",
    "df_1 = df_1.reset_index(drop=True)\n",
    "\n",
    "\n",
    "apoe_df = apoe_df[apoe_df['Participant'].isin(relevant_ids)]\n",
    "apoe_df = apoe_df.reset_index(drop=True)\n",
    "\n",
    "# setting some nan-values to valid numbers:\n",
    "\n",
    "# DMSMDL0 -> 13000  (had one na)\n",
    "\n",
    "missing_DMSMDL0 = df_1['DMSMDL0'].isnull()\n",
    "missing_ind_DMSMDL0 = missing_DMSMDL0[missing_DMSMDL0].index\n",
    "df_1.loc[missing_ind_DMSMDL0, 'DMSMDL0'] = 13000\n",
    "\n",
    "# DMSMDL4 -> 20000 (had one na)\n",
    "\n",
    "missing_DMSMDL4 = df_1['DMSMDL4'].isnull()\n",
    "missing_ind_DMSMDL4 = missing_DMSMDL4[missing_DMSMDL4].index\n",
    "df_1.loc[missing_ind_DMSMDL4, 'DMSMDL4'] = 20000\n",
    "\n",
    "# RVPA -> 0.6 (had 8 na)\n",
    "missing_RVPA = df_1['RVPA'].isnull()\n",
    "missing_ind_RVPA = missing_RVPA[missing_RVPA].index\n",
    "df_1.loc[missing_ind_RVPA, 'RVPA'] = 0.5\n",
    "\n",
    "# RVPMDL -> 1500 (had 8 na)\n",
    "missing_RVPMDL = df_1['RVPMDL'].isnull()\n",
    "missing_ind_RVPMDL = missing_RVPMDL[missing_RVPMDL].index\n",
    "df_1.loc[missing_ind_RVPMDL, 'RVPMDL'] = 1300\n",
    "\n",
    "\n",
    "len(relevant_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_removed1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the PSN:\n",
    "- Scale the data\n",
    "- calculate similarity and adjacency matrix\n",
    "- Implement the PSN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 100 # seed for spring layout algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_sc = scale_df(df_1.copy(), cols_var, 'minmax')\n",
    "#df_1_sc = scale_df(df_1.copy(), cols_var, 'standard') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_stsc = scale_df(df_1.copy(), cols_var, 'standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjM_V1_eucl = makeAdjMatrix(df_1_sc[cols_var], 'Eucl', 'V1', save_adjM=False)\n",
    "#adjM_V3_eucl = makeAdjMatrix(df_3_sc[cols_var], 'Eucl', 'V3', save_adjM=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjM_V1_eucl_st = makeAdjMatrix(df_1_stsc[cols_var], 'Eucl', 'V1', save_adjM=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSN_euclV1, similaritiesV1 = create_PSN(adjM_V1_eucl, 0.465, s, 'CANTAB: PSN of scores (v1) for all (n=342), 45 variables, eucl.dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarities(similaritiesV1, 'Distribution of euclidean similarities for 45 variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSN_euclV1, similaritiesV1_st = create_PSN(adjM_TEST, 0.141, s, 'CANTAB: PSN of scores (v1) for all (n=342), 45 variables, eucl.dist, standardscaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarities(similaritiesV1_st, 'Distribution of euclidean similarities for 45 variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### V1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjM_V1_cos_st = makeAdjMatrix(df_1_sc[cols_var], 'Cos', 'V1 (st.scaled)', save_adjM=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjM_V1_cos = makeAdjMatrix(df_1_sc[cols_var], 'Cos', 'V1', save_adjM=True)\n",
    "#adjM_V3_cos = makeAdjMatrix(df_3_sc[cols_var], 'Cos', 'V3', save_adjM=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSN_cosV1, sim_cosV1 = create_PSN(adjM_V1_cos, 0.944, s, 'CANTAB: PSN of scores (v1) for all (n=346), 45 variables, cos.sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarities(sim_cosV1, 'Distribution of cosine similarities for 45 variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard V1 scaled df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSN_cos_st, sim_cos_st = create_PSN(adjM_V1_cos_st, 0.416, s, 'Cantab: PSN for V1 for all (n=342), 45 var, cos.sim, standard scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarities(sim_cos_st, 'Distribution of cosine similarities for 45 variables, st scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pearson correlation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### V1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjM_V1_pears = makeAdjMatrix(df_1_sc[cols_var], 'Pears', 'V1', save_adjM=True)\n",
    "#adjM_V3_pears = makeAdjMatrix(df_3_sc[cols_var], 'Pears', 'V3', save_adjM=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSN_pearsV1, sim_pearsV1 = create_PSN(adjM_V1_pears, 0.933, s, 'CANTAB: PSN of scores (v1) for all (n=346), 45 variables, pears.sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarities(sim_pearsV1, 'Distribution of pearson corr. similarities for 45 variables')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
